{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
    
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62944072",
        "outputId": "7ba35a24-5761-4403-ca4f-14a531588965"
      },
      "source": [
        "%%shell\n",
        "echo \"Hello from the shell!\"\n",
        "pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the shell!\n",
            "/content\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# whisper for speech recognition\n",
        "# OPENAI supported speech to text model, but we will not use this one due to some error I cant figure out\n",
        "# i will use whisper model but the one hosted on the hugging face transformers library. This will allow to be create pipelines for model analysis and transcription\n",
        "import whisper\n",
        "#vosk is a open source , offline speech recognition library( so does not need internet and is lightweight)\n",
        "# Vosk uses the kaldi engine, integrating speech models into apps, and comes with pretrained models\n",
        "#kaldi is a  toolkit for speech recognit and dev. provides algos for building asr systems\n",
        "#kaldirecognizer is an instance within vosk framework for doing the transcription of audio\n",
        "from vosk import Model as VoskModel, KaldiRecognizer\n",
        "# for opening, reading and writing wav files\n",
        "import wave\n",
        "#pyTorch for wav2vev2, helpful with these neural network models im using\n",
        "import torch\n",
        "#wav2vev2 model and tokenizer from HuggingFace\n",
        "# hugging face is central repo for pretained models, hugging face provides the wav2vec2 model and the tokenizer\n",
        "# Wav2Vec2 is speech to text model and self supervised speech model  that learns patterns from raw audio.pretrained on lots of unlabeled audio data, covering various speeches.\n",
        "#this model is used with CTC( connectionist temporal classification) . CTC is extra layer to the model , where model learns to predict seq of chars from the extracted features.\n",
        "# the tokenizer converts model output into seq of tokens ( characters or subwords). Decodes output into readable text.\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "#for word error rate , lower the better\n",
        "from jiwer import wer\n",
        "import time\n",
        "import numpy as np\n",
        "# i will test 3 asr models : whisperAI, vosk and wav2vev2. for each 3 metrics are measured : WER ( how diff models transripts is from ground truth)\n",
        "# 0.0 means perfect transcription. HigherWER --> worse . Lower wer --> accurate model . WER = (number of insertions + deletions + substitutions) / (number of words in ground truth)\n",
        "#time is measured ( how long model took to transcribe audo file from start to finish  of prediction process)\n",
        "# transcription text metric : acc string of text produced by model for audio input.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6PuH0tmED_JA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hello, my name is Amrish. I'm currently a fourth year studying computer science at North carolina state university. Two years ago in 2023, AI and machine learning got super important in tech, healthcare, and even education. In this project, I will be trying to prototype a ASR Model that begins with English to English conversion. Then I will extend the model to support Tamil to English conversion. Finally I will try Sourashtra to English. Saurashtrians are a linguistic and cultural community in Tamil Nadu, India, originally from the Saurashtra region of Gujarat, who speak the Indo-Aryan Saurashtra language.\n",
        "\n",
        "This text may be good because it has a good mix of numbers, commas, case sensitiive names and grammar a good ASR model should know how to transcribe.\n"
      ],
      "metadata": {
        "id": "e7QRqxzzTcv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#testing asr model accuracy\n",
        "from google.colab import files\n",
        "print(\"Upload an .wav audio file\")\n",
        "uploaded = files.upload()\n",
        "#turning the uploaded files ( dict) into numpy list( the whisper pipelines expect numpy array as input), and picks first one ( assuming you uploaded 1 file)\n",
        "audio_file = list(uploaded.keys())\n",
        "if len(audio_file) > 0 :\n",
        "  audio_path = audio_file[0]\n",
        "else :\n",
        "  print(\"No file uploaded\")\n",
        "  audio_path = \" \"\n",
        "print(audio_path)\n",
        "\n",
        "#get ground truth transcript so that we can compare model output with acc results to achieve metrics\n",
        "ground_truth = input(\"paste actual transcript for the audio: \")\n",
        "#clean up spaces and lower case\n",
        "ground_truth = ground_truth.strip().lower()\n",
        "print(\"Ground truth:\", ground_truth)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "WnEdvJIxKjuX",
        "outputId": "9ac1b408-d964-4561-aec2-c37ec529b67c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload an .wav audio file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3971c637-8afb-40e9-8b31-63c6ec5830ae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3971c637-8afb-40e9-8b31-63c6ec5830ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ASRAudioTesting.wav to ASRAudioTesting (1).wav\n",
            "ASRAudioTesting (1).wav\n",
            "paste actual transcript for the audio: Hello, my name is Amrish. I'm currently a fourth year studying computer science at North carolina state university. Two years ago in 2023, AI and machine learning got super important in tech, healthcare, and even education. In this project, I will be trying to prototype a ASR Model that begins with English to English conversion. Then I will extend the model to support Tamil to English conversion. Finally I will try Sourashtra to English. Saurashtrians are a linguistic and cultural community in Tamil Nadu, India, originally from the Saurashtra region of Gujarat, who speak the Indo-Aryan Saurashtra language.\n",
            "Ground truth: hello, my name is amrish. i'm currently a fourth year studying computer science at north carolina state university. two years ago in 2023, ai and machine learning got super important in tech, healthcare, and even education. in this project, i will be trying to prototype a asr model that begins with english to english conversion. then i will extend the model to support tamil to english conversion. finally i will try sourashtra to english. saurashtrians are a linguistic and cultural community in tamil nadu, india, originally from the saurashtra region of gujarat, who speak the indo-aryan saurashtra language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing  whisperAI ASR model with timer\n",
        "print(\"testing whisperAI model\")\n",
        "#loading pre trained whisper model - base model (speed and accurate)\n",
        "import whisper\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "start_time = time.time()\n",
        "#transcribe text\n",
        "whisper_output = whisper_model.transcribe(audio_path)\n",
        "# result is a dict, this is getting the value ( the transcribed text) from the 'text' key having the transcript\n",
        "# gets the main transcribed text\n",
        "transcribed_text = whisper_output[\"text\"]\n",
        "#if \"text \" key not in dict, the transcribed_text set to empty string\n",
        "if (\"text\" not in whisper_output):\n",
        "  transcribed_text = \" \"\n",
        "\n",
        "#removes leading spaces and converts to lowercase.\n",
        "transcribed_text = transcribed_text.strip().lower()\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "#calc wer rate against ground truth\n",
        "whisper_wer = wer(ground_truth, transcribed_text)\n",
        "print(\"\\n[Whisper]\")\n",
        "print(\"WER:\", round(whisper_wer, 2))\n",
        "print(\"Time:\", round(elapsed_time, 2), \"seconds\")\n",
        "print(\"Transcription:\", transcribed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "6KxZXeuXQ4Fu",
        "outputId": "596ad272-9196-4e42-9d66-6471b6216fb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing whisperAI model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'whisper' has no attribute 'load_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3431703262.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#loading pre trained whisper model - base model (speed and accurate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwhisper_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#transcribe text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'whisper' has no attribute 'load_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing wh#isper ai model (via hugging face)\n",
        "#pipeline function gives easy acess to pre trained models hosted on hugging face\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "#loading whisper ai using hugging face pipeline, \"automatic-speech-recognition\" is task name, second pam is the model we want pipeline to use ,\n",
        "#third param means output will include time segments for each phrase ( allows for long form generation )\n",
        "whisper_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", return_timestamps=True)\n",
        "\n",
        "start_time = time.time()\n",
        "# transcribe the audio file, whisperpipeline called with audio path returns dict with \"text\" holding transcription data\n",
        "transcribed_text = whisper_pipeline(audio_path)[\"text\"]\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# clean up transcribed text\n",
        "transcribed_text = transcribed_text.strip().lower()\n",
        "\n",
        "# Calculate WER\n",
        "# Ensure ground_truth variable is available from previous cells\n",
        "whisper_wer = wer(ground_truth, transcribed_text)\n",
        "print(\"\\n[Whisper (transformers)]\")\n",
        "print(\"WER:\", round(whisper_wer, 2))\n",
        "print(\"Time:\", round(elapsed_time, 2), \"seconds\")\n",
        "print(\"Transcription:\", transcribed_text)\n",
        "\n",
        "#diff between this whisper model and previous is that in previous, i used the original OpenAI Whisper library\n",
        "# using pip install openai-whisper. Now I want to extend this model to tamil again to see its accuracy on Tamil\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660,
          "referenced_widgets": [
            "1f219b13a41847bc9d2a12fddbaa6c0a",
            "507ebdab0cd84908acde95b0f050c0f3",
            "b1e0a2f989844b45b7044b35ae9b8218",
            "55efcfae370643b3ace2487a8d96e5e4",
            "8ba98031df8e492abf387554726151dd",
            "1b18fb26695f4186bca21dff79068461",
            "2be6feaf937f4173a2537b32c55135ee",
            "e54bed427f5240df8cea5a9981990e80",
            "a60baf08f8d3458390e1f68a861798ae",
            "29f148eda70944be9dfa88bf2fc3bab7",
            "701dd8fb21d0444bb3e6695693c5eb1f",
            "5128c77c2b4345359778ba10ab251ca5",
            "cf899f21a7f84525ae733010513b802f",
            "f8668955580549399204e62948534214",
            "41d6f45bb2d24cf39cc9f953ffae814c",
            "2d59f85f39724e7aadf08da29b4b85ee",
            "a3255702542a4849b18b4272d703be2e",
            "c6b485d8a1cc428bb4a3568b6e18ee7c",
            "26920fcf21a04603954a21701f6f9099",
            "573900eb4b3e441ead298a0790eaffa0",
            "9101240f950644fd80b97d1844fd89e9",
            "4042661e180b4595a3ce6c8ccb693be3",
            "eeddc9adfca44a3b9a66696ab40ac611",
            "9311bb80c2dc40b4b22772ff79a97dcb",
            "26ae6198a03c4f78925c890c407d2330",
            "14d10ab4b16a425a95ffda9cda38d1ad",
            "e6754a159f16488eaac5f8c7e37709b6",
            "a41c560959804e24ae556676db382611",
            "040e5e51d17f42ff97c4a27ddc524ad0",
            "abf0a590065043e0af9f057523976498",
            "989cedf118004d6fa45e3f1717536afb",
            "bc1fa3486b63490182befc38b9d99627",
            "bbce42438d0d49d7a0fc073e68539b24",
            "006c25f55efa4d508dd628d6cdf61da3",
            "49e90d344ba74020942ece6df78945a5",
            "8cbac913a0c14057b7db78aef7e4c3d8",
            "e8adb169cc074b71ab1be00733c48a5b",
            "33b3d82a5b924d70b461dff40aa77c2f",
            "a441cf41bbac4926bae2dad9cf78c64f",
            "960fbb99678642f8b24771d6057a36ef",
            "8d4d30d3c02c4fbaaf3ca010702fd9e7",
            "788704c5c3a749c38a5fdb9902553609",
            "f5208aa8bf0d488c85dd43fdac063f56",
            "4431da2421ed4275afa243b2b90606a5",
            "be169c0bf56546ce9fa47f60bd9cb9df",
            "628087dd82c54a9a8898e83dc17a7ffb",
            "8a7df94ce9264587a521693c98c2bb3d",
            "90be5682d80d41caba0e152be47a606d",
            "5c782e3fe1bf4c04adb8362e52f0699e",
            "b42e0cfa21a9497d9e3b91be52661cbf",
            "6b011e44f4784981a9a7e0406484ea1a",
            "ffcc7f8f7fe24ee9a2a8cb42cc1361d0",
            "08f1b09c6ba94ae1b83233350928afac",
            "4dcb2d0d4dc3430d82eb377408225ccd",
            "e7cc1c1bef32449f8fad19f09e5ff74a",
            "da78f783772e4cbcaf9df65b2a5dd6a5",
            "934e18c8d2b144a9b42f938650d64e55",
            "1a6de6da85034a2999dde53f165ec528",
            "dfb659c475ae4c3d87f62a9c571e013c",
            "d82a3e8cae7f459f9d4396b424c9afaf",
            "0ecae5c8b2b74000b0e314808d8256ba",
            "c2117f7efbd64ffa959c08428a1be541",
            "d3d150bcab5044ad8396f815c3818ce4",
            "03e4ba96b18c413eb57a632b3138f29c",
            "357795c4994840a6abc1758befee2e46",
            "1b95def9316c4a0aac15e6dec4f33f57",
            "46ba0f5eeef34053a334b2fb4333d364",
            "af09ea46f9894b098a72ab45d309b883",
            "bf9a04b0bb494e759bda7b39cc9011a1",
            "6b2b3753cdc34f5baaf11593f500aee9",
            "75701205dd8643faa24ea38c2705cea7",
            "cc6d5ec5d31a4f72a7ac0a88db16ccef",
            "9ee50d3ca86044ff8009b803b6e460e0",
            "898b4e22bc0f4b978fdbe8ed8dc4694c",
            "f1ab20f8688a4cc49f22719f8a5acdf7",
            "828160def616485789c117504f2dfc68",
            "ce7caabc67cc489db7e799d0a8c59e8c",
            "f48686b7efde4c17b8bf9a4daf0791f9",
            "b529ba517f744bc3b95e7bb6c9224941",
            "b1f6c43e48f74138bf7c6b629f0e7c64",
            "0f712854f7ca4d89972eb0a241fcab9d",
            "9acac1eba67f4746ad97d6a6c017eead",
            "ea78a173d5c7419fa9f457962d95b217",
            "607d8b9c78504c58baa794ded2924fea",
            "5295cfcee4694841ab4a2703b6352863",
            "89cd6e7dd5b3458e8fd43d66da4a0644",
            "8c34a63b6f314932839f3ceb611b67d6",
            "d5b5b86224cd4c48814292c5d898d57d",
            "d00b051100cb41dabaae6242ab0082b5",
            "646c863d92624229a8252aae52081d83",
            "bfeb06eadb9240dd99385707682d85e4",
            "4d380b4351f64f35a6fe9eb3253cda9c",
            "c511a6904add48a5b701a152f0cf65d9",
            "66290ba8a41f44b19d80ef64971ccedb",
            "e9440c8d3f004af4aee37142c041e77c",
            "658aa79cc74342deb2596052a1046779",
            "57468d15477c4437a0dbb02582615177",
            "02132cb46e964c76864c2a1635473a92",
            "bebcb77e3cdc4e5199bb71209ef68c94",
            "b9f22c56adfc4059a4cb6d69b1ca7e1f",
            "e65402ee164e47c2befe2943a3e6891c",
            "9022e071598343a7ba699a8e0c60052a",
            "9827b348bf6d4103a51c8f539e7de51f",
            "de3ec743eec843829035c6fe2236b87c",
            "20a13aded20b4781b8146b7dd33cc77a",
            "65e8278ac9df419991a5fcab6d4ae422",
            "5d78617c4c9a4a1a8affbc00939d7ece",
            "ef97403282d049569d92539268fa4e90",
            "5f24e86f07af4876abf8524ac8f6b28d",
            "1e8d540aeb974c32af591f1330e62fa6",
            "7beabc092fa54e93870b1b955c12c242",
            "00924262dd964e978252ff173ab7edad",
            "8969bf93dd554b6d88d10bfae9d143e6",
            "b1ab33dedb9145b485536c8306227338",
            "7e4ae7aea1ea46b48266281d2848c508",
            "00d3327921ba436e903e6afbe0629a66",
            "eef381801f05465fa1369bb063ec8cf0",
            "073f3e71dbf545aebbf347c7fde218eb",
            "5879f54ba9f74b6aa9f882b523c2b33a",
            "baddee9d26e8455fa8ad041c69fa324f",
            "2f7e17cc66ae4825a39f0c0e48958858"
          ]
        },
        "id": "3R1cdOXqnDGN",
        "outputId": "5de8cbcf-e346-4997-eeb1-1852c2f44494"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f219b13a41847bc9d2a12fddbaa6c0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5128c77c2b4345359778ba10ab251ca5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eeddc9adfca44a3b9a66696ab40ac611"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "006c25f55efa4d508dd628d6cdf61da3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be169c0bf56546ce9fa47f60bd9cb9df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da78f783772e4cbcaf9df65b2a5dd6a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46ba0f5eeef34053a334b2fb4333d364"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "normalizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f48686b7efde4c17b8bf9a4daf0791f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d00b051100cb41dabaae6242ab0082b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9f22c56adfc4059a4cb6d69b1ca7e1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7beabc092fa54e93870b1b955c12c242"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Whisper (transformers)]\n",
            "WER: 0.05\n",
            "Time: 47.8 seconds\n",
            "Transcription: hello, my name is amrish. i'm currently a fourth year studying computer science at north carolina state university. two years ago in 2023, ai and machine learning got super important in tech, healthcare, and even education. in this project, i will be trying to prototype a asr model that begins with english to english conversion. then i will extend the model to support tamil to english conversion. finally, i will try sarashtra to english. sarashtrians are a linguistic and cultural community in tamil nadu, india, originally from the sarashtra region of gujarat, who speak the indo-aryan sarashtra language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the tamil to english grouth truth\n",
        "\tஅவனை எரிக்கும் அதே நெருப்பு என்னையும் எரிக்கட்டும் என்று விம்மி விம்மி அழுது அலுத்துக்கூறினான்.\tகீழே விழுந்துவிடாமல் தம்மை அடிக்கடி தூக்கி நிறுத்திக் கொண்ட மனிதசாதி கெட்டதைப் போல செடிகள் உலகம் கெடவில்லை.\n",
        "  to english reads : \"Let the same fire that burns him burn me too,\" he cried, wearily.\n",
        "The world of plants is not as bad as the human race, which has repeatedly picked itself up without falling down."
      ],
      "metadata": {
        "id": "ne2kwYl8Bkwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#attempting to use tamil for whisper ai\n",
        "#found tamil speech audio on Mozilla common voice, 62.7 mb --> TAMIL TO TAMIL ASR\n",
        "#short audio , 17 secs of audio the truth text is above this code block\n",
        "#takes tamil audio file , transcribes to tamil text using whisper and prints accuracy metrics\n",
        "from google.colab import files\n",
        "from transformers import pipeline\n",
        "from jiwer import wer\n",
        "import time\n",
        "import re # Import the re module\n",
        "\n",
        "\n",
        "print(\"Upload an .wav audio file\")\n",
        "uploadedT = files.upload()\n",
        "#turning the uploaded files ( dict) into list, and picks first one ( assuming you uploaded 1 file)\n",
        "audio_fileT = list(uploadedT.keys())\n",
        "if len(audio_fileT) > 0 : # Corrected variable name here\n",
        "  tamil_path = audio_fileT[0]\n",
        "else :\n",
        "  print(\"No file uploaded\")\n",
        "  tamil_path = \" \"\n",
        "print(tamil_path)\n",
        "\n",
        "#extra step to clean texts, removing punc, normalize spacing , lower case\n",
        "def normalize_text(text):\n",
        "  text = text.lower()\n",
        "  # Remove punctuation\n",
        "  text = re.sub(r'[^\\w\\s\\u0B80-\\u0BFF]', '', text)\n",
        "  # Normalize whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text.strip()\n",
        "\n",
        "\n",
        "#get ground truth transcript so that we can compare model output with acc results to achieve metrics\n",
        "ground_truthT = input(\"paste actual tamil transcript for the audio: \")\n",
        "#clean up spaces and lower case\n",
        "ground_truthT = normalize_text(ground_truthT)\n",
        "print(\"Ground truth:\", ground_truthT)\n",
        "\n",
        "#load whisper pipeline for asr tamil to tamil\n",
        "#whisper auto detects language and transcribes to original ( tamil in this case)\n",
        "whisper_pipelineTamil = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-base\", #better accuracy\n",
        "    return_timestamps=True,\n",
        "    generate_kwargs={\"language\": \"ta\", \"task\": \"transcribe\"} # pass language and task in generate_kwargs, telling to transcribe tamil audio to tamil text by defualt\n",
        ")\n",
        "print(\"Transcribing tamil audio\")\n",
        "start = time.time()\n",
        "result = whisper_pipelineTamil(tamil_path)\n",
        "transcribed_text = normalize_text(result[\"text\"])\n",
        "end= time.time()\n",
        "tamilTime = end- start\n",
        "\n",
        "tamil_wer = wer(ground_truthT, transcribed_text)\n",
        "print(\"\\n[Whisper Tamil → Tamil]\")\n",
        "print(\"WER:\", round(tamil_wer, 2))\n",
        "print(\"Time:\", round(tamilTime, 2), \"seconds\")\n",
        "print(\"Transcription:\", transcribed_text)\n",
        "\n",
        "#interesting to see that even though both translatoins are diff, the wer is still the same , both .91\n",
        "# wer blind to meaning - only counts word - level matches , not if sentence makes sense or closer meaning\n",
        "# from chat ai - if both outputs are similarly incorrect (i.e., require lots of edits), you can get the same (or similar) high WER values—even if the content and words are different\n",
        "# with google translate , I can try BLEU( bilingual evaluation understudy)- extra metric  as it measures overlap in phrases and meaning better\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "-1Lz4-nF9jcC",
        "outputId": "9e81656b-c3a4-4b4f-990e-da6533e0c041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload an .wav audio file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a4d07ae5-88b0-45f0-bd84-aa2ba11785cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a4d07ae5-88b0-45f0-bd84-aa2ba11785cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tamilExtension.wav to tamilExtension (14).wav\n",
            "tamilExtension (14).wav\n",
            "paste actual tamil transcript for the audio: அவனை எரிக்கும் அதே நெருப்பு என்னையும் எரிக்கட்டும் என்று விம்மி விம்மி அழுது அலுத்துக்கூறினான். கீழே விழுந்துவிடாமல் தம்மை அடிக்கடி தூக்கி நிறுத்திக் கொண்ட மனிதசாதி கெட்டதைப் போல செடிகள் உலகம் கெடவில்லை.\n",
            "Ground truth: அவனை எரிக்கும் அதே நெருப்பு என்னையும் எரிக்கட்டும் என்று விம்மி விம்மி அழுது அலுத்துக்கூறினான் கீழே விழுந்துவிடாமல் தம்மை அடிக்கடி தூக்கி நிறுத்திக் கொண்ட மனிதசாதி கெட்டதைப் போல செடிகள் உலகம் கெடவில்லை\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing tamil audio\n",
            "\n",
            "[Whisper Tamil → Tamil]\n",
            "WER: 1.08\n",
            "Time: 35.25 seconds\n",
            "Transcription: கிழே விழந்து விடமல் தம்மை அடிக் கடி தூகே நிருத்திக்கொண்டு மனிதா சாதிக் கட்டதை போல செடிக்கல் உலகம் கடவில்லை அவனையிருக்கும் அதே நிருப்பு என்னையும் விரிக்கட்டும் என்று விம்பி விம்பி அழுத்து அழுத்து கூறினான்\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#translate tamil transcription to english\n",
        "# this way translates the transcribed tamil to englsih using hugging face pipeline\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "print(\"\\nTranslating Tamil transcription to English...\")\n",
        "#this translation pipeline will translate the transcripted tamil text above to english\n",
        "translation_pipeline = pipeline(\n",
        "    \"translation\",\n",
        "    model=\"facebook/nllb-200-distilled-600M\", # this is the multilangual translation model\n",
        "    src_lang=\"tam_Taml\", #source lang\n",
        "    tgt_lang=\"eng_Latn\" # output lang - target\n",
        ")\n",
        "start_trans = time.time()\n",
        "# tamil text is passed in as input and pipeline processes this text using the pretrained translation model from facebook\n",
        "# [0] since pipeline can return list of possible translations, 0 returns most likely translation from results\n",
        "translated_english = translation_pipeline(transcribed_text)[0]['translation_text']\n",
        "end_trans = time.time()\n",
        "translation_time = end_trans - start_trans\n",
        "\n",
        "print(\"[Tamil → English Translation]\")\n",
        "print(\"Translation:\", translated_english)\n",
        "print(\"Translation Time:\", round(translation_time, 2), \"seconds\")\n",
        "\n",
        "#computing wer on englsih ground truth as well\n",
        "ground_truthE = input(\" Paste the actual English translation for the audio (for WER): \").strip().lower()\n",
        "def normalize_en(text):\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\u0B80-\\u0BFF]', '', text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "    #text = text.lower()\n",
        "    #text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    #text = re.sub(r'\\s+', ' ', text)\n",
        "    #return text.strip()\n",
        "#running our truth text and translated text through a normalization algo so that wer comparison is possible, and that puncuation and extra chars don't mess with the asr model\n",
        "ground_truthE = normalize_en(ground_truthE)\n",
        "translated_english_norm = normalize_en(translated_english)\n",
        "english_wer = wer(ground_truthE, translated_english_norm)\n",
        "print(\"English WER:\", round(english_wer, 2))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vVsdfPCPzAH",
        "outputId": "90909c8d-5ab6-4e85-afb8-ec2cacb3cd08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translating Tamil transcription to English...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tamil → English Translation]\n",
            "Translation: Wimpy Wimpy said, \"The flowering world doesn't pass away like a human race without falling to the ground\".\n",
            "Translation Time: 22.35 seconds\n",
            " Paste the actual English translation for the audio (for WER): \"Let the same fire that burns him burn me too,\" he cried, wearily. The world of plants is not as bad as the human race, which has repeatedly picked itself up without falling down.\n",
            "English WER: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this way does tamil to english translation using google trans library,instead of hugging face library\n",
        "from googletrans import Translator\n",
        "translator =  Translator()\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "print(\"Translating Tamil transcription to English...\")\n",
        "start_trans = time.time()\n",
        "#using google translate API to translate Tamil to english, #src is tamil and dest is english\n",
        "#translate method returns Translation obj. by doing .text on this object, we can the translate text as a string\n",
        "translated_englishGT = translator.translate(transcribed_text, src='ta', dest='en').text\n",
        "end_trans = time.time()\n",
        "translation_time = end_trans - start_trans\n",
        "\n",
        "print(\"[Tamil → English Translation]\")\n",
        "print(\"Translation:\", translated_english)\n",
        "ground_truthEGT = input(\"\\n(Optional) Paste the actual English translation for the audio (for WER): \").strip().lower()\n",
        "#normalization algo as before\n",
        "def normalize_en(text):\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\u0B80-\\u0BFF]', '', text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "    #text = text.lower()\n",
        "    #text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    #text = re.sub(r'\\s+', ' ', text)\n",
        "    #return text.strip()\n",
        "\n",
        "#computing wer between the translated english and the truth english conversion of our tamil audio ( same as before )\n",
        "ground_truthEGT = normalize_en(ground_truthEGT)\n",
        "translated_english_normGT = normalize_en(translated_englishGT)\n",
        "english_wer = wer(ground_truthE, translated_english_normGT)\n",
        "print(\"English WER:\", round(english_wer, 2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCev-QkqQ-8f",
        "outputId": "e3bb8f8b-6d32-4a03-e89c-2a4c6abe35b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translating Tamil transcription to English...\n",
            "[Tamil → English Translation]\n",
            "Translation: The Sedical world is not like a man who is a caste, and he will not fall on the east, and he will spread the same seredenness to me.\n",
            "\n",
            "(Optional) Paste the actual English translation for the audio (for WER): \"Let the same fire that burns him burn me too,\" he cried, wearily. The world of plants is not as bad as the human race, which has repeatedly picked itself up without falling down.\n",
            "English WER: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tune whisper , above I just tested with a small set of input audios\n",
        "# i should upload a plethora of audio files in tamil to train and potentially tune, helping with domain or accent adaption\n",
        "#creating a dataset with columns audio and transcript in tamil, allowing me to create parallel Tamil English datasets\n",
        "# from this file, I can also populate hindi , english and eventually sourashtra translations of the audio\n",
        "import pandas as pd\n",
        "import os\n",
        "from pydub import AudioSegment # audio conversion\n",
        "from deep_translator import GoogleTranslator # Import GoogleTranslator from deep_translator\n",
        "\n",
        "#reading in the validated.tsv file ( this file has meta deta about the audio files. I have linked these audio files in the clips directory here)\n",
        "# using the tsv file and its cols (the path and transcript ), I will make output df , so I can prepare the resulting dataframe for model training\n",
        "\n",
        "df = pd.read_csv(\"validated.tsv\", sep=\"\\t\")\n",
        "if df is not None:\n",
        "    # check if the 'clips' directory exists\n",
        "    clips_dir = \"clips\"\n",
        "    if not os.path.exists(clips_dir):\n",
        "        print(f\"Directory not found: {clips_dir}\")\n",
        "    else:\n",
        "        #output dir for wavs\n",
        "        wav_clips_dir = \"wav_clips\"\n",
        "        os.makedirs(wav_clips_dir, exist_ok=True)\n",
        "        #convert mp3 to wav , create lists for dataset, this will hold full paths and transcripts for each tamil audio\n",
        "        audio_paths = []\n",
        "        transcripts = [] # tamil\n",
        "        engTranscripts = [] # english\n",
        "        hindiTranscripts = [] # hindi\n",
        "\n",
        "        # Initialize the translator outside the loop for efficiency\n",
        "        translator_en = GoogleTranslator(source='ta', target='en')\n",
        "        translator_hi = GoogleTranslator(source='ta', target='hi')\n",
        "\n",
        "        # loop will go through each entry in the tsv file and extract the path name\n",
        "        for index, row in df.iterrows():\n",
        "            mp3_file = os.path.join(clips_dir, row[\"path\"])\n",
        "            #construct input mp3 and output is wav\n",
        "            wav_file = os.path.join(wav_clips_dir, row[\"path\"].replace(\".mp3\", \".wav\"))\n",
        "\n",
        "            #convert only if wav does not already exist\n",
        "            if not os.path.exists(wav_file):\n",
        "                try:\n",
        "                    #load the mp3 , set sample rate ( covert to mono) and export the file as wav\n",
        "                    audio = AudioSegment.from_file(mp3_file)\n",
        "                    audio = audio.set_frame_rate(16000).set_channels(1)  # 16kHz mono\n",
        "                    audio.export(wav_file, format=\"wav\")\n",
        "                except FileNotFoundError:\n",
        "                     print(f\"Mp3 file not found: {mp3_file}\")\n",
        "                     continue # skip this file and continue with the next\n",
        "                except Exception as e:\n",
        "                    print(f\"Error converting {mp3_file} to WAV: {e}\")\n",
        "                    continue # skip on other errors too\n",
        "\n",
        "            # i am populating the audio path and transcripts list\n",
        "            audio_paths.append(wav_file)\n",
        "            #row will be dict ( keys represent the column name)and sentence key holds the sentence value for that particular audio file\n",
        "            transcripts.append(row[\"sentence\"])\n",
        "\n",
        "            #translate to english and hindi using deep_translator\n",
        "            try:\n",
        "                eng_translation = translator_en.translate(row[\"sentence\"])\n",
        "                hindi_translation = translator_hi.translate(row[\"sentence\"])\n",
        "                engTranscripts.append(eng_translation)\n",
        "                hindiTranscripts.append(hindi_translation)\n",
        "            except Exception as e:\n",
        "                print(f\"Error translating sentence for row {index}: {e}\")\n",
        "                engTranscripts.append(\"\") # Append empty string on error\n",
        "                hindiTranscripts.append(\"\") # Append empty string on error\n",
        "\n",
        "\n",
        "        #now we can build dataframe and save as dataset with tamil audio and its tamil translation as specified in validation.tsv\n",
        "        dataset_df = pd.DataFrame({\n",
        "            \"audio\": audio_paths,\n",
        "            \"transcript_Tamil\": transcripts,\n",
        "            \"transcript_English\": engTranscripts,\n",
        "            \"transcript_Hindi\": hindiTranscripts\n",
        "        })\n",
        "\n",
        "        dataset_df.to_csv(\"tamil_asr_dataset.csv\", index=False)\n",
        "        print(\"✅ Dataset saved as tamil_asr_dataset.csv\")\n",
        "        print(dataset_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU4byapnrnb_",
        "outputId": "6ea8465c-0d23-436e-c6b2-55cf94534d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset saved as tamil_asr_dataset.csv\n",
            "                                    audio  \\\n",
            "0  wav_clips/common_voice_ta_37342302.wav   \n",
            "1  wav_clips/common_voice_ta_37311038.wav   \n",
            "2  wav_clips/common_voice_ta_37311039.wav   \n",
            "3  wav_clips/common_voice_ta_37311133.wav   \n",
            "4  wav_clips/common_voice_ta_37318886.wav   \n",
            "\n",
            "                                    transcript_Tamil  \\\n",
            "0  அதன்படி மறுநாள் காலை சென்னைக்கு வந்து சேர்ந்தோம்.   \n",
            "1  அவனை எரிக்கும் அதே நெருப்பு என்னையும் எரிக்கட்...   \n",
            "2  கீழே விழுந்துவிடாமல் தம்மை அடிக்கடி தூக்கி நிற...   \n",
            "3  இங்ஙனம் செயற்படுத்த இயலாவண்ணம் நிற்போருக்கு யா...   \n",
            "4  காரைத்தான் அப்பா காலையிலே எடுத்துக் கொண்டு போவ...   \n",
            "\n",
            "                                  transcript_English  \\\n",
            "0  Accordingly we arrived in Chennai the next mor...   \n",
            "1  Wimmy Wimmy cried and said, \"Let me burn the s...   \n",
            "2  The world is not as bad as the humanitarian ev...   \n",
            "3  Who is the companion of the people who are una...   \n",
            "4  It was only then that I remembered that the ca...   \n",
            "\n",
            "                                    transcript_Hindi  \n",
            "0                तदनुसार हम अगली सुबह चेन्नई पहुंचे।  \n",
            "1  विम्मी विम्मी ने रोते हुए कहा, \"मुझे वही आग जल...  \n",
            "2  दुनिया उतनी बुरी नहीं है जितनी कि मानवीय बुराई...  \n",
            "3  उन लोगों का साथी कौन है जो इसे करने में असमर्थ...  \n",
            "4    यह तभी था जब मुझे याद आया कि कार सुबह ली गई थी।  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict, Audio\n",
        "#Dataset- fund data struct in hugging face datasets library. a single collection fo data samples ( aduio files,transcripts,computer vision, etc).\n",
        "#DatasetDict - dict like object used to hold multiple datast objects. after splitting dataset into test and training, we have\n",
        "# a dataset dict using the train and test dataset.\n",
        "#Audio - when casting audio file path to Audio feature, datasets lib knows how to load audio file from path, decode it and store as numerical arrray ( numpy or pytorch tensor)\n",
        "#resamples audio to specific rate, important here since whisper wants input sample rate as 16000 Hz and mono\n",
        "\n",
        "#https://huggingface.co/blog/fine-tune-whisper  ---> this link was what i used to help with fine tuning of whisper ai model\n",
        "# in this link, the author used a big hindi dataset for training, but I'm using something much smaller for time issues( cant wait 5 hours for model to train as shown in article)\n",
        "# the point of this code chunk is to run my tamil parallel csv ( the one with audio, tamil, english, hindi)\n",
        "# against the base open ai whisper asr model.\n",
        "\n",
        "# load  your custom CSV\n",
        "df = pd.read_csv(\"tamil_asr_dataset.csv\")\n",
        "df = df.rename(columns={\"transcript_Tamil\": \"sentence\"})  # standardizing col name for consistency\n",
        "\n",
        "# convert the pandas dataframe to hugging face dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "#here since input audio is sampled at 48khz, we need to downsample to 16, cast_column signals to dataaset to resample audio files on the fly as they are loaded\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000)) #telling datasets lib to treat audio col as\n",
        "#audio data. when column assessed later, lib will load audio file from path in each row, decode it , and reample to sample rate of 16000 hZ\n",
        "# Audio feature stores processed audio data as numerical array within dict struct.\n",
        "\n",
        "# Split into train/test\n",
        "ds = dataset.train_test_split(test_size=0.3, seed=42)\n",
        "print(ds)\n",
        "\n",
        "#this is the baseline wer before fine tuning my asr model using the tamil_asr_dataset.csv\n",
        "#pipeline is way to use pre trianed models for tasks, like ASR , without model loading\n",
        "#pipelines are objects that abstract complex code from transformers lib , making it easy to work with asr models\n",
        "from transformers import pipeline\n",
        "import evaluate ##evaluate lib provides standard metrics for eval these models( WER)\n",
        "\n",
        "# Load the original pre-trained Whisper pipeline (no fine-tuning) . uses the whisper-small model and its tokenizer\n",
        "#pipeline take audio input and produce text transcription using pre trained weights of model\n",
        "asr_pipe = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-small\",\n",
        "    tokenizer=\"openai/whisper-small\"\n",
        ")\n",
        "\n",
        "# Run predictions on your test set\n",
        "#two lists to store models predicted results and ground truth transcription\n",
        "predictions = []\n",
        "references = []\n",
        "#for loop that will run the model on the specified test data in my dataset\n",
        "# target value is the transcript_Tamil and the prediction by the model is the predicted value\n",
        "for item in ds[\"test\"]:\n",
        "    # extracts audio data as numerical array from audio feature , datasetslib, because you cast column to Audio, has already handled loading and resampling the audio to 16kHz\n",
        "    audio_input = item[\"audio\"][\"array\"] # acc array containing audio samples is under the key \"array\" within \"audio\" feature, numpy array has the audio waveform data\n",
        "    #extracts truth text for this particular audio\n",
        "    reference_text = item[\"sentence\"]\n",
        "    # pretrained whisper model loaded via the asr_pipeline pipleine processes audio input. performs asr and returns result\n",
        "    result = asr_pipe(audio_input)\n",
        "    #returned result is a dict, we acess the text key to get transcribed text for that audio\n",
        "    predicted_text = result[\"text\"]\n",
        "    #append prediction and reference to lists for metric comparsion\n",
        "    predictions.append(predicted_text)\n",
        "    references.append(reference_text)\n",
        "\n",
        "# loading  WER metric from evaluate library\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "#compares each predicted setence to its corresponding ref sentence , using the equation\n",
        "# (insertions, deletions, or substitutions) needed to change prediction to be = to reference, divided by #of words in ref\n",
        "baseline_wer = 100 * wer_metric.compute(predictions=predictions, references=references)\n",
        "print(\"Baseline WER (no fine-tuning):\", baseline_wer)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "da008590d0c34afcac4ac836e49e6f20",
            "0d61abe55f60454d8ed12a44ac8276f7",
            "767a7b08a2fa41e2b5a9fafdbc8118c4",
            "5ff360a6b9c14c61ae3fa1aa75e707cf",
            "4495a3315251424ea4042f48597307b1",
            "3fedc98b7bdf47d99aebb6e42b25340b",
            "020f20034048461980b6b3244f6b6028",
            "97bac5280b25469cbe06c7ecaff40e01",
            "efac479b389e44a4bd051f228bd917df",
            "e88c178ee66b4753a1ee2ed13b87d793",
            "b425d4f01e244cbba55a5f28c7eb02ab"
          ]
        },
        "id": "2tRtZxHk4gv2",
        "outputId": "5f09d5ed-51e9-4560-892b-c8a12df44d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'sentence', 'transcript_English', 'transcript_Hindi'],\n",
            "        num_rows: 9\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'sentence', 'transcript_English', 'transcript_Hindi'],\n",
            "        num_rows: 4\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da008590d0c34afcac4ac836e49e6f20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline WER (no fine-tuning): 58.82352941176471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# whisper is Transformer based encoder-decoder model( seq to seq model). Maps a seq of audio spectrogram features to a sequence of text tokens\n",
        "#1. raw audio inputs are converted to log-Mel spectogram by action of feature extractor. The Transformer encoder then encodes the spectrogram to form a sequence of encoder hidden states\n",
        "# Finally, the decoder autoregressively predicts text tokens, conditional on both the previous tokens and the encoder hidden states.\n",
        "\n",
        "#in this article, it says 680,000 hours of labelled pre-training data, Whisper models demonstrate a strong ability to generalise to many datasets and domain\n",
        "# these pre trained checkpoints have 3% wer on the test-clean subset of LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (c.f. Table 8 of the Whisper paper)\n",
        "# the asr knowledge acquired thru whisper during pre trinaing can be used with my low level resource lang: sourashtra -->thru fine tuning, i can adapt these pre-trained checkpoints\n",
        "\n",
        "#whisper is pre trained and fine tuned ;trained  to correctly classify the target text token from a pre-defined vocabulary of text tokens\n",
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "\n",
        "#transformers to load and train our whisper model\n",
        "# soundfile to pre process audio files, evaluate and jiwer for performance, tensorboard to log metrics.\n",
        "\n",
        "# dataset was split in training and testing in above chunk\n",
        "\n",
        "#asr pipeline is decomposed into 3 parts : a feature extractor which pre processes raw audio inputs\n",
        "#, model which performs the seq to seq mapping , tokenizer which post processes the model outputs to text format\n",
        "\n",
        "#WhisperFeatureExtractor: speech is a 1d dim array that varies with time.  value of array at time setp is the signals amplitude.\n",
        "#speech is cont, so it has inf number of amptlitude values. interval in whcih we sample our audio is known as\n",
        "# sampling rate and is measured in hz. whisper expects audio inputs with rate of 16khz.\n",
        "#extractor has two operations: first pads/truncuates batch of audio samples such that all samples have input len of 30s, samples shroter is padded with 0, longer is truncuated\n",
        "#second operation is that extractor coverts padded audio arrays to log mel spectorgrams( spectograms are visual reps of freq of a signal)\n",
        "#the spectogram is the what the whsiper model encoder expects as input\n",
        "#Spectrogram (input to Encoder) ➡️ Encoder Processing ➡️ Encoder Hidden States ➡️ Cross-Attention Mechanism in Decoder ➡️ Decoder Processing (using encoder states and previous tokens) ➡️ Predicted Text Token (output of Decoder)\n",
        "#WhisperTokenizer\n",
        "#Whisper uses BPE (Btye - pair encoding) for tokenization. This method breaks down words into smaller subword units,\n",
        "#allowing the tokenizer to handle rare or out-of-vocabulary words effectively\n",
        "#whisper model outputs text tokens that indicate index of predicted text among the dict of vocab terms( model has list of all possible tokens it can use) --> the integer id represnets the index of predicted result in the tamil vocab\n",
        "# tokenizer maps a seq of text index ids to acc text string : e.g. [1169, 3797, 3332] -> \"the cat sat\") --> converts text like tamil trasncirpt into seq of numerical token ids that the whisper model decoeder understans and vice versa\n",
        "\n",
        "#normally, when we  use encoder only models for asr, we decode using Connectionist Temporal Classification (CTC).\n",
        "# whisper tokenizer already pre trained on transcriptions of 96 pre training lang- including tamil/hindi. we load the\n",
        "# tokenizer by specifying target language and task.\n",
        "model_name = \"openai/whisper-small\"   # or base/medium if you want\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
        "#these  arguments inform the tokenizer to prefix the language and task tokens to the start of encoded label sequences:\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name, language=\"Tamil\", task=\"transcribe\")\n",
        "# to simplify using feature extractor and tokenizer, im wrapping both into single WhisperProcessor class. we need to keep track of two objects during training: processor and model\n",
        "#processor used to simplify the steps to prepare audio for the model, and decode the predicted ID’s back into text.\n",
        "#use processor to handle audio ninput and text labels\n",
        "processor = WhisperProcessor.from_pretrained(model_name, language=\"Tamil\", task=\"transcribe\")\n",
        "#encoding --> tokenizer appends special tokens to start /end of seq, such as lang token, task token\n",
        "#=-- processes entire input spectogram into representation that captures  phonetic and linguistic information--> produces seq of encoder hidden states( summary of audion content )\n",
        "#decoding --> when decoding label ids, we can skip these speical tokens, allowing us to return string of original input form\n",
        "#-- decoder takes encoder hidden states and seq of previsouly predicted text tokens as input --> generates output text transcription, tok by tok\n",
        "#predidicts the next token based on the input audion , we need predicted text tokens to to understand the text decoder  has already generated\n",
        "# decoeder uses cross attention to use the info from encoder hidden states , at teach step while its making a token,\n",
        "# the decoder looks at diff parts of encoder input( the hidden states and seq of token ids it has already generated )to find info relevant to predicting current text toek ( decoder can hear the relev parts of audion to create transcription)\n",
        "# then tokenizer maps seq of ids back to human readable text\n",
        "# since the direct output of decoder in model is seq of numerical token IDS( each toekn in the vocab assigned a uniqe int id)\n",
        "# converseley, when tokenizer converts text into numerical form, it breaks text into tokens and replaces each token with id\n",
        "\n",
        "#preparing data ready for model\n",
        "#load and resample audio data , we use feature extractor to compute logmel spectogram input features from 1-dim audio file\n",
        "# seems like each sample of the audio in thee file is the batch\n",
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    #calling Extractor with audio array and its sampling rate.\n",
        "    # feature_extractor returns dict , and acc spectogram features are stored under input_features key, since extractor is procesisng\n",
        "    # a batch, input_features is a list, and [0] acesses features for the first in the batch( in our case we only one sample in the batch processed func ATM)\n",
        "    #resulting spectogram features added to the batch dict under new key \"input_features\"\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    # encode target text to label ids( numerical token ids which model decoder and training process expect for target labels)\n",
        "    # we are acessing ground truth for curent sample, .input_ids accesses seq of token ids generated by tokenizer and resulting token id added to batch dict\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n",
        "# this applies prepare_dataset function to dataset ds( the training and testing splits), applying to each batch or sample of data\n",
        "#remove_columns tells map to remove original columns from ds after function has been applied.\n",
        "#audio, sentence no longer needed , since we transformed to input_features and labels columns\n",
        "ds = ds.map(prepare_dataset, remove_columns=ds[\"train\"].column_names)\n",
        "\n",
        "#training and evaluation, The Trainer class helps us here: provides an API for feature- completing in pytorch\n",
        "#goes hand in hand with TrainingArguments class, which gives wide range of options to customize how a model is trained\n",
        "\n",
        "# we first have to load pre trained checkpoint and configure it correctly for training, define a data collator( this takes our pre procssed data and prepares pytorch tensors ready for model)\n",
        "#since we want wer metric, we need a compute_metric function that handles this computation, we define training args( used by trainer in making training scheduel)\n",
        "# we then eval on test data to see how accuratly we have trained it to transcribe speech to tamil\n",
        "import torch\n",
        "\n",
        "#pretrained weights are the learned params (weights and biases) of a Neural network that has already been trained on large datasets\n",
        "from transformers import WhisperForConditionalGeneration # this class used when conditional generation tasks like ASR where model creates text conditional on the audion input\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name) # laods model arch and its pretrained weightsfrom hugging face hub\n",
        "model.generation_config.language = \"tamil\" #tells model when generating text , should aim to produce tamil text,helping model uses its internal knowledge about the lang\n",
        "model.generation_config.task = \"transcribe\" # tells model that task is to transcribe audio ( speech to text in same lang)\n",
        "model.generation_config.forced_decoder_ids = None #forces decoder to generate specific tokens at start of generated seq , None means no tokens are forcced, allowing decoder to decide starting seqeunce\n",
        "#based on input audio, lan and task\n",
        "\n",
        "#data collator take list of indiv data samples from dataset and combine them into single batch, ensuring all seq in batch have same len (padding)\n",
        "#data collator for a seq-seq speech model is unique in that treats the input_features and labels independently: the input_features must be handled by the feature extractor and the labels by the tokenizer.\n",
        "# input features already padded to 30s, we just have to convert log mel spectogram to Pytorch tensor.\n",
        "#labels are unpadded - first pad seq to max len in batch using tokenizer .pad method. padding tokens replaced with -100, tokens are not taken into account when computing loss\n",
        "#then cut start of transcript token from the beginning of the label sequence as we append it later during training.\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "  #class expects a processor object, and int repping token id that signals start of a seq for decoder\n",
        "    processor: any\n",
        "    decoder_start_token_id: int\n",
        "    #constuctor for the class\n",
        "    def __init__(self, processor, decoder_start_token_id):\n",
        "        self.processor = processor\n",
        "        self.decoder_start_token_id = decoder_start_token_id\n",
        "    #When Trainer needs to create batch of data, it will call this method and pass list of data samples ( dicts, each represnting one sample) to the features argument\n",
        "    #this method then processes features to create padded batch\n",
        "    def __call__(self, features):\n",
        "        #creates a list of dict, where each dict contains the spectogram features (\"input_features\") for one sample from input features list\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        #after preparing data for padding by feature extractor, we use the pad method of the feature_extractor acessed via the processor to pad\n",
        "        #input_features so that all spectograms in batch have same len, return_tensors = \"pt\" tells to return res as Pytorch tensors\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\") # padded batch of spectograms are stored in batch dict\n",
        "        #get list of dict having tokenized label ids (labels) for each sample, prepping for padding\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        #uses pad method of tokenizer to pad label_features so that all label sequences have same len, result as pytorch tensor\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "        #replaces padding tokens (which are at positions where the attention_mask is not 1) with -100 , these are igored\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        #hthis checks to see if first token in all label sequences in the batch is the decoder start tokem\n",
        "        #if it is remove the token, because the decoder implicity starts with start token and you dont need to predict it again as first label\n",
        "        #we can cut start of transcript token from beginning of label seq as we append it later during training\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "        batch[\"labels\"] = labels\n",
        "        # batch dict has padded input features and labels\n",
        "        return batch\n",
        "#creates instacne of the above class.Passes processor object and token ID that signifies begining of a seq for the decoder\n",
        "# this object is then given to hugging face trainer to handle batch creation during training\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=tokenizer.bos_token_id\n",
        ")\n",
        "\n",
        "\n",
        "#using wer for metric, need func that takes our model predictions and retursn wer metric.\n",
        "import evaluate\n",
        "metric = evaluate.load(\"wer\")\n",
        "#replaces -100 with the pad_token id in the label_ids( undoing the step we applied in the data collator to ignore padded tokens correclty in loss)\n",
        "\n",
        "def compute_metrics(pred):\n",
        "\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    #during data preparation with data collaotr, padding tokens in labels converted to -100 so loss func ignore it\n",
        "    #now, we decode lables back into text for we so we need to replace -100 values back with the acc padding token id used by tokenizer\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    #converting predicted and label ids into list strings ; skip_spcial_tokens= true is used to ignore special tokens ( padding tokens, start/end tokens) during decoding process, so you just get transcribed text\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    #comparing predicted string vs the reference string( truth variable)\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    #retuns dict containing calc wer, Trainer wants metric to be returned in dict format\n",
        "    return {\"wer\": wer}\n",
        "#Trainer class abstracts complexities of machine larning training loop, instead of manually writing code to\n",
        "#Iterating through epochs Processing batches of data Performing forward passes through the model Calculating the loss,\n",
        "#Trainer takes care of all this. You config the trainer by providing it with model you want to train, training args, and the test/trianing datasets, and func to compute metrics\n",
        "\n",
        "# in final step, we define all params related to training;  init Seq2SeqTrainer with theseqence to sequence model, data and arguments, starting the training and saving trained model/processor\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "# various hyperparams, the params are xplained in the article\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-tamil-finetuned\",\n",
        "    per_device_train_batch_size=2,          # Small batch size, fits on most GPUs/Colab\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=1,          # No need for accumulation with small batch\n",
        "    num_train_epochs=10,                    # More epochs since you have so little data\n",
        "    learning_rate=5e-5,                     # Slightly higher LR to learn quickly\n",
        "    save_steps=10,                          # Save frequently (not really needed here)\n",
        "    eval_steps=5,                           # Evaluate frequently\n",
        "    logging_steps=1,                        # Log every step for visibility\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    fp16=False,                             # Use True only if you have a GPU with FP16\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"no\",                     # Don't waste time saving checkpoints\n",
        "    report_to=[],                           # No TensorBoard/W&B unless you want logs\n",
        "    load_best_model_at_end=False,           # Not needed for tiny debug run\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    # Remove push_to_hub, warmup_steps, gradient_checkpointing for simplicity\n",
        ")\n",
        "#Imports Trainer class for seq- seq models\n",
        "from transformers import Seq2SeqTrainer\n",
        "# creating instance of Trainer, providing the training arguments, whisper model instance you loaded earlier,\n",
        "#training/eval dataset, the data collator you defined to prepare batches, func to define wer, tokenizer object\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,  #looks wrong but that what is shown on article, --> should be changed to processor.tokenizer\n",
        ")\n",
        "#starts the training process process. Trainer will run the training loop for specified number of epochs or steps\n",
        "trainer.train()\n",
        "#trainer uses the data collator to handle batch creation and padding\n",
        "\n",
        "trainer.save_model(\"./whisper-tamil-finetuned\")\n",
        "processor.save_pretrained(\"./whisper-tamil-finetuned\")\n",
        "\n",
        "#weird that fine tuning does not reduce the wer rate. we probably need more data to be trained on , but I can't wait hours\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889,
          "referenced_widgets": [
            "b3e512dcfbde4c1798454acdd1ea843e",
            "d7d3644309bd4040baae8f79603569f5",
            "2080e57e98724536bf16e789092c6d86",
            "80e02ced3f784a04904fa34a23d28649",
            "edb91d682475461fa9e1c09b8a20ab6d",
            "72c97153109343e2bcb04bcdeee45121",
            "0ebdc2ee34fa4d109bc7cc33cd7338f5",
            "9cfee2daf884474eb4897d402f893315",
            "50a70f2a5b4647bc9f37d045348b1487",
            "f42c60dc3d474bcea4323cb90fb5b412",
            "52e0aa3404d3405ab1a69e06722416cc",
            "64c38dc0ad5c4b98ad7118033580f9e5",
            "aa10ce9377b245efb519254ad5246942",
            "fc47424b066f4d479aeccd3316acb127",
            "9e94451d46924aa9b6674da82d6dc9ae",
            "33d543e7a056475b8646ed6d2f4a6194",
            "a9f1b56fafd14ab4b9b5d79a049d6bc7",
            "ab91223f598e47419a5bbedf3ca7125e",
            "afc91f4042ca48408638f8d4ac4a5294",
            "a5b98b33162642d6b32cea0ac378d2a4",
            "250b9a0e18fd4a5db8ba18c95e592e89",
            "ab4a3c882b654aa78a9892294df531d8"
          ]
        },
        "id": "JsOKhayXZOQs",
        "outputId": "a9ca8971-04b0-44af-be07-f7db9cb5b08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3e512dcfbde4c1798454acdd1ea843e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64c38dc0ad5c4b98ad7118033580f9e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-14-215312517.py:184: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/50 31:23 < 12:55, 0.02 it/s, Epoch 7/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.178900</td>\n",
              "      <td>0.927745</td>\n",
              "      <td>67.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.257400</td>\n",
              "      <td>0.724700</td>\n",
              "      <td>147.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.249900</td>\n",
              "      <td>0.673844</td>\n",
              "      <td>67.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.236100</td>\n",
              "      <td>0.703254</td>\n",
              "      <td>79.411765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.100800</td>\n",
              "      <td>0.680304</td>\n",
              "      <td>76.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.087000</td>\n",
              "      <td>0.674514</td>\n",
              "      <td>76.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.068000</td>\n",
              "      <td>0.679513</td>\n",
              "      <td>82.352941</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 48:48, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.178900</td>\n",
              "      <td>0.927745</td>\n",
              "      <td>67.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.257400</td>\n",
              "      <td>0.724700</td>\n",
              "      <td>147.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.249900</td>\n",
              "      <td>0.673844</td>\n",
              "      <td>67.647059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.236100</td>\n",
              "      <td>0.703254</td>\n",
              "      <td>79.411765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.100800</td>\n",
              "      <td>0.680304</td>\n",
              "      <td>76.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.087000</td>\n",
              "      <td>0.674514</td>\n",
              "      <td>76.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.068000</td>\n",
              "      <td>0.679513</td>\n",
              "      <td>82.352941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.050300</td>\n",
              "      <td>0.664164</td>\n",
              "      <td>88.235294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.061900</td>\n",
              "      <td>0.644890</td>\n",
              "      <td>91.176471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.045500</td>\n",
              "      <td>0.632502</td>\n",
              "      <td>88.235294</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3465: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script takes a Tamil audio file, translates and transcribes it to English text using Whisper, and prints out accuracy metrics.\n",
        "\n",
        "from transformers import pipeline\n",
        "from jiwer import wer\n",
        "import time\n",
        "\n",
        "# ---- Upload Tamil audio (for Colab, can modify for local use) ----\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"Upload a .wav Tamil audio file:\")\n",
        "    uploaded = files.upload()\n",
        "    tamil_path = list(uploaded.keys())[0]  # Get the filename of the uploaded file\n",
        "except ImportError:\n",
        "    # For non-Colab/local runs\n",
        "    tamil_path = input(\"Enter path to your Tamil .wav file: \")\n",
        "\n",
        "# ---- Get the ground-truth transcript (in English) ----\n",
        "ground_truth_english = input(\"Paste the correct English translation for the audio: \").strip().lower()\n",
        "\n",
        "# ---- Load Whisper pipeline for translation (Tamil-to-English) ----\n",
        "print(\"Loading Whisper pipeline for Tamil-to-English translation...\")\n",
        "whisper_pipeline = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-base\",  # You can use 'large' for better accuracy\n",
        "    return_timestamps=True,\n",
        "    #generate_kwargs={\"task\": \"translate\", \"language\": \"ta\"}\n",
        ")\n",
        "\n",
        "print(\"Transcribing and translating audio (Tamil → English)...\")\n",
        "start_time = time.time()\n",
        "result = whisper_pipeline(tamil_path)\n",
        "translated_text = result[\"text\"].strip().lower()\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# ---- Show results and WER metric ----\n",
        "print(\"\\n[Whisper Tamil → English]\")\n",
        "print(\"Transcription/Translation:\", translated_text)\n",
        "print(\"Ground Truth:\", ground_truth_english)\n",
        "print(\"Time:\", round(elapsed_time, 2), \"seconds\")\n",
        "if ground_truth_english:\n",
        "    trans_wer = wer(ground_truth_english, translated_text)\n",
        "    print(\"WER:\", round(trans_wer, 2))\n",
        "else:\n",
        "    print(\"No ground truth provided; WER not computed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "KJvmLAwTFpyI",
        "outputId": "ce500add-56b6-4913-94a5-9e9a5d18c3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload a .wav Tamil audio file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ef809fac-0b03-4b51-b101-dd491651177c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ef809fac-0b03-4b51-b101-dd491651177c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tamilExtension.wav to tamilExtension (8).wav\n",
            "Paste the correct English translation for the audio: \"Let the same fire that burns him burn me too,\" he cried, wearily. The world of plants is not as bad as the human race, which has repeatedly picked itself up without falling down.\n",
            "Loading Whisper pipeline for Tamil-to-English translation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing and translating audio (Tamil → English)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Whisper Tamil → English]\n",
            "Transcription/Translation: he is the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the\n",
            "Ground Truth: \"let the same fire that burns him burn me too,\" he cried, wearily. the world of plants is not as bad as the human race, which has repeatedly picked itself up without falling down.\n",
            "Time: 84.25 seconds\n",
            "WER: 12.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Wav2Vec2 ASR MODEL\n",
        "#getting model and tokenizer from HuggingFace repo\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model_w2v = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "#soundfile lib used for reading / writing to audo files\n",
        "import torchaudio\n",
        "#two values returned from read() : the audio data itself ( as array) and sample rate of audio file\n",
        "speech, rate = torchaudio.load(audio_path)\n",
        "# audio data returned as tensor, multi dim array in PyTorch and sample rate ( audio samples/ sec)\n",
        "speech_tensor, rate = torchaudio.load(audio_path)\n",
        "# removing dimensions from the tensor that have a size of 1\n",
        "speech = torch.squeeze(speech_tensor, 0)\n",
        "#converts pytorch tensor back into numpy array\n",
        "speech = speech.numpy()\n",
        "\n",
        "# need to resample audio rate to 16KHz if not 16KHz, since Wav2Vec2 expects input audio at a 16kHz sample rate\n",
        "# can use torch audio or librosa ( used librosa elsewhere, wanted to use differnet mechanism here)\n",
        "if rate != 16000:\n",
        "  #creating ReSample object from torchaudio ( original rate and indicate wanted rate)\n",
        "  resampler = torchaudio.transforms.Resample(orig_freq=rate, new_freq=16000)\n",
        "  speech_tensor = resampler(speech_tensor) #changes sample rate\n",
        "  #converting tensor back into numpy array, stored as speech\n",
        "  speech = speech_tensor.squeeze().numpy()\n",
        "  rate = 16000\n",
        "\n",
        "#tokenize\n",
        "# takes speech audio as input , \"pt \" tells tokenizer to return output as Pytorch tensors\n",
        "#longest padding : pads audio arrays to same len ( if you wer proccesing multiple files at once, this would\n",
        "# pad shorter seqs to len of longest one, not needed here since only one audio file; req arg)\n",
        "input_values = tokenizer(speech, return_tensors=\"pt\", padding=\"longest\").input_values\n",
        "\n",
        "#inference with the model - we dont need gradients ( not training, just inference); speeds up compute time, saves mem\n",
        "with torch.no_grad():\n",
        "  start = time.time()\n",
        "  #pass audio input to model to get raw output scores, logits( the models initial predictions)\n",
        "  #logits represent model confidence for each possible token ( chars or subwords) at each point in time in audio\n",
        "  logits= model_w2v(input_values).logits\n",
        "  end = time.time()\n",
        "  w2v_time = end - start\n",
        "\n",
        "  # argmax finds index of the maximum value along specified dim\n",
        "  #dim = -1 , argmax is to be done on last dim of logits tensor,  in asr model output, the last dim\n",
        "  # represents diff possible tokens model can predict. Goes through the logits for each point in time\n",
        "  #and finds index of the token with highest score ( most likely token). result is sequence of indices showing the\n",
        "  # most probable tokens predicted by model for audio input.\n",
        "  predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "  #decode list of predicted token ids into readable text\n",
        "  #batch_decode converts list of token ids back into  text - just one result\n",
        "  w2v_text_list = tokenizer.batch_decode(predicted_ids)\n",
        "  if (len(w2v_text_list) > 0):\n",
        "    #since i processed single file, list should have one transcribed string\n",
        "    w2v_text = w2v_text_list[0]\n",
        "  else :\n",
        "    w2v_text = \" \"\n",
        "\n",
        "#cleanup\n",
        "w2v_text = w2v_text.strip().lower()\n",
        "  #calc wer rate\n",
        "w2v_wer = wer(ground_truth, w2v_text)\n",
        "print(\"\\n[Wav2Vec2]\")\n",
        "print(\"WER:\", round(w2v_wer, 2))\n",
        "print(\"Time:\", round(w2v_time, 2), \"seconds\")\n",
        "print(\"Transcription:\", w2v_text)\n",
        "\n",
        "#example of logits for my understanding\n",
        "#at some moment, model \"confidence scores\" (logits) for three possible letters ('a', 'b', 'c') are:\n",
        "# 'a' = 0.5, 'b' = 2.1 ,'c' = -1.0. here, logits for'b' (2.1) is the highest.\n",
        "# we have torch.argmax([0.5, 2.1, -1.0], dim=-1) , result would be index of highest value which is 1\n",
        "# in our Wav2Vec2  model, we have scores for many possible tokens ( entire vocab) and for many points in time across\n",
        "# the audio. arg max picks most likely token at each point in time based on scores.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_MaV58MVdBs",
        "outputId": "8023dc97-c2b0-437e-b6cd-0f27b89dd10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
            "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:720: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Wav2Vec2]\n",
            "WER: 0.33\n",
            "Time: 18.61 seconds\n",
            "Transcription: hello my name is amrich i'm currently a fourth year studying computer science at north carolina state university two years ago in twenty twenty three a i and machine learning got superimportant intech health care and even education in this project i will be trying to prototype a s ar model that begins with english to english conversion then i will extend the model to support tamel to english conversion finally i will try serastra to enguish serastrians are a linguistic and cultural community in tamel nadu india originally from the serastr region of gugerat who speak the indoary and serastra language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Vosk asr model\n",
        "!wget -q https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
        "!unzip -q vosk-model-small-en-us-0.15.zip\n",
        "!ls vosk-model-small-en-us-0.15\n",
        "#import librosa to change sampling rate to supported rate for vosk\n",
        "import librosa\n",
        "#import soundfile for reading/writing audio files\n",
        "import soundfile as sf\n",
        "inputAudio = audio_path\n",
        "#load data\n",
        "data, sampleRate = sf.read(inputAudio)\n",
        "#coverting to mono if needed\n",
        "if len(data.shape) > 1:\n",
        "  data = librosa.to_mono(data.T)\n",
        "#resample to 16khz if needed\n",
        "if sampleRate != 16000:\n",
        "  data = librosa.resample(data, orig_sr=sampleRate, target_sr=16000)\n",
        "#save as PCM 16 bit wav\n",
        "sf.write(\"converted.wav\", data, 16000, subtype='PCM_16')\n",
        "audio_path = \"converted.wav\"\n",
        "# path downloaded vosk model folder\n",
        "vosk_model_path = \"vosk-model-small-en-us-0.15\"\n",
        "#loads acc lang model from the said path that holds the model files into memory\n",
        "vosk_model = VoskModel(vosk_model_path)\n",
        "#open audio file for r : reading and  b means open in binary mode.\n",
        "wf = wave.open(audio_path, \"rb\")\n",
        "# you pass the loaded vosk_model into recognizer so it knows which lang model to use\n",
        "#getnchannels checks if num of channels is not 1 ( not mono audio )  and getsamwidth checks if sample\n",
        "# width ( bytyes / sample) is not 2. getcomptype checks if the compression type is not NONE\n",
        "# vosk expects uncompressed audio data ( PCM fomrat)\n",
        "if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
        "    print (\"Audio file must be WAV format mono PCM\")\n",
        "    wf.close()\n",
        "    voskText = \" \"\n",
        "    voskTime = \"N/A\"\n",
        "else :\n",
        "  #init vosk recognizer( performs actual speech recognition) with model and sample rate from file\n",
        "  vosk_recognizer = KaldiRecognizer(vosk_model, wf.getframerate()) # getframerate() as the second param gets sample rate of the audio.\n",
        "  allText = [] # to keep up with chunks of recognized text\n",
        "  #partialResults = []\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  #read audio until the  end\n",
        "  while True:\n",
        "    #read chunk of audio data ( 4000 audio frames - frame has samples of channels at given point in time )\n",
        "    data = wf.readframes(4000)\n",
        "    if len(data) == 0:\n",
        "      break # EOF\n",
        "    #feeding data into recgonizer for processing in increments.\n",
        "    #returns true if it has produced a partial result ready to be retreived,\n",
        "    ok = vosk_recognizer.AcceptWaveform(data)\n",
        "    if ok:\n",
        "      #Result() returns json string that has partial transcription\n",
        "      partialRes = vosk_recognizer.Result()\n",
        "      print(\"Partial result:\", partialRes)\n",
        "      #extract text from the partial result\n",
        "      try :\n",
        "        text = json.loads(partialRes)[\"text\"]\n",
        "        if text :\n",
        "          allText.append(text)\n",
        "      except Exception as e:\n",
        "        print(\"error in partial result\",e)\n",
        "        text = \"\"\n",
        "\n",
        "  #add final recognition result\n",
        "  # after all audio data processed, a json string having complete and final transcription is appended,\n",
        "  # list of  partial results + final result all appended to be one string\n",
        "  finalRes = vosk_recognizer.FinalResult()\n",
        "\n",
        "  import json\n",
        "  #extract text from  json result, attempts to add final result( can be empty since sometimes, finalResult() returns only what was not already included in the partials)\n",
        "  #catches errors with the voskResult\n",
        "  try :\n",
        "    txt = json.loads(finalRes).get(\"text\", \"\")\n",
        "    if txt:\n",
        "      allText.append(txt)\n",
        "  except Exception as e:\n",
        "    print(\"error in final result\", e)\n",
        "\n",
        "  end = time.time()\n",
        "  voskTime = end - start\n",
        "\n",
        "  #join recgonized chunks into one full transcript ( cleaned up )\n",
        "  voskText = \" \".join(allText)\n",
        "  voskText = voskText.strip().lower()\n",
        "\n",
        "\n",
        "#results of metrics\n",
        "vosk_wer = wer(ground_truth, voskText)\n",
        "print(\"\\n[Vosk]\")\n",
        "print(\"WER:\", round(vosk_wer, 2))\n",
        "print(\"Time:\", round(voskTime, 2), \"seconds\")\n",
        "print(\"Transcription:\", voskText)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBaUAL3zd-wy",
        "outputId": "80c65573-9926-4275-d726-00e50564f6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace vosk-model-small-en-us-0.15/am/final.mdl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/graph/disambig_tid.int? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/graph/HCLr.fst? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/graph/Gr.fst? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/graph/phones/word_boundary.int? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/conf/model.conf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/conf/mfcc.conf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/ivector/splice.conf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/ivector/final.dubm? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/ivector/global_cmvn.stats? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/ivector/final.ie? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/ivector/online_cmvn.conf? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/ivector/final.mat? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace vosk-model-small-en-us-0.15/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "am  conf  graph  ivector  README\n",
            "Partial result: {\n",
            "  \"text\" : \"hello my name is i'm rich\"\n",
            "}\n",
            "Partial result: {\n",
            "  \"text\" : \"i'm currently a fourth year studying computer science at north carolina state university\"\n",
            "}\n",
            "Partial result: {\n",
            "  \"text\" : \"two years ago and twenty twenty three a and machine learning got super important and tech healthcare and even education\"\n",
            "}\n",
            "Partial result: {\n",
            "  \"text\" : \"in this project i will be trying to prototype of a us our model that begins with english to english conversion\"\n",
            "}\n",
            "Partial result: {\n",
            "  \"text\" : \"then i will extend the model to support tamil to english conversion\"\n",
            "}\n",
            "Partial result: {\n",
            "  \"text\" : \"finally i will try suresh dirty english suresh trans or a linguistic and cultural community in tamil nadu india originally from the so rushed her region of good to rot who speak the into wary and so rushed her language\"\n",
            "}\n",
            "\n",
            "[Vosk]\n",
            "WER: 0.41\n",
            "Time: 7.82 seconds\n",
            "Transcription: hello my name is i'm rich i'm currently a fourth year studying computer science at north carolina state university two years ago and twenty twenty three a and machine learning got super important and tech healthcare and even education in this project i will be trying to prototype of a us our model that begins with english to english conversion then i will extend the model to support tamil to english conversion finally i will try suresh dirty english suresh trans or a linguistic and cultural community in tamil nadu india originally from the so rushed her region of good to rot who speak the into wary and so rushed her language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Google speech to text api\n",
        "from google.cloud import speech\n",
        "#credentials.json\n",
        "import os\n",
        "import wave\n",
        "\n",
        "#env variable that tells google libraries where to find service account credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/credentials.json\"\n",
        "\n",
        "# client that can talk with Google cloud speech to text api\n",
        "client = speech.SpeechClient()\n",
        "#audio file path ( mono , 16 bit 16000Hz WAV for best results)\n",
        "\n",
        "#read audio data into mem, opens file in binary mode\n",
        "with open(audio_path, \"rb\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Get the sample rate from the WAV file header\n",
        "try:\n",
        "    with wave.open(audio_path, 'rb') as wf:\n",
        "        audio_sample_rate = wf.getframerate()\n",
        "except wave.Error as e:\n",
        "    print(f\"Error reading WAV file header: {e}\")\n",
        "    audio_sample_rate = 16000 # default to 16000 if reading fails\n",
        "\n",
        "# set audio and recognition config\n",
        "#tells google what audio you are sending , the audio data is wrapped in a RecognitionAudio object\n",
        "audio = speech.RecognitionAudio(content=content)\n",
        "# the configuration for recognition\n",
        "config = speech.RecognitionConfig(\n",
        "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16, # PCM WAV encoding\n",
        "    sample_rate_hertz= audio_sample_rate,\n",
        "    language_code=\"en-US\", # english\n",
        ")\n",
        "#call api\n",
        "start = time.time()\n",
        "\n",
        "#sends audio and config as a request to google api and waits for response, recognize() returns response object with transcripts\n",
        "response = client.recognize(config=config, audio=audio)\n",
        "end = time.time()\n",
        "totalTime = end - start\n",
        "\n",
        "#extract transcription\n",
        "googleText = \" \"\n",
        "#since google may return multiple results, we have to have a for loop where for each result\n",
        "# we take the most likely and combine them\n",
        "for result in response.results:\n",
        "    #alternatives[0] is the top (most confident) transcription for this element as thought by the api , so the first alternative\n",
        "    #transcript looks at acc text string of the transcription of that alt\n",
        "    googleText += result.alternatives[0].transcript + \" \"\n",
        "googleText = googleText.strip().lower()\n",
        "google_wer = wer(ground_truth, googleText)\n",
        "print(\"\\n[Google Speech-to-Text]\")\n",
        "print(\"WER:\", round(google_wer, 2))\n",
        "print(\"Time:\", round(totalTime, 2), \"seconds\")\n",
        "print(\"Transcription:\", googleText)\n",
        "\n",
        "#now I have got audio to Text, so asking user toupload a .wav file and having a truth value\n",
        "# then i ran my audio data through the google speech to text api model to get predictions. metrics down below\n",
        "# Now, i want to sport live speech, so letting user speak and converting this to text in english\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fJFSQLCqfkk",
        "outputId": "45263476-60b4-4fc4-bbce-76c16a950893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Google Speech-to-Text]\n",
            "WER: 0.23\n",
            "Time: 9.12 seconds\n",
            "Transcription: hello my name is amish  i'm currently a fourth-year studying computer science at north carolina state university  2 years ago in 2023 ai and machine learning got super important in tech healthcare and even education  in this project i will be trying to prototype a asr model that begins with english to english conversion  then i will extend the model to support tamil to english conversion  finally i will try to rational to english  sarians are a linguistic and cultural community in tamil nadu india originally from the sasha region of gujarat who speak the indo-aryan sastre language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ae7ea02"
      },
      "source": [
        "!pip install deep_translator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "41af3159",
        "outputId": "706571a6-676c-4b55-ee75-947160c86a00"
      },
      "source": [
        "#fine tune whisper , above I just tested with a small set of input audios\n",
        "# i should upload a plethora of audio files in tamil to train and potentially tune, helping with domain or accent adaption\n",
        "#creating a dataset with columns audio and transcript in tamil, allowing me to create parallel Tamil English datasets\n",
        "# from this file, I can also populate hindi , english and eventually sourashtra translations of the audio\n",
        "import pandas as pd\n",
        "import os\n",
        "from pydub import AudioSegment # audio conversion\n",
        "from googletrans import Translator\n",
        "\n",
        "#reading in the validated.tsv file ( this file has meta deta about the audio files. I have linked these audio files in the clips directory here)\n",
        "# using the tsv file and its cols (the path and transcript ), I will make output df , so I can prepare the resulting dataframe for model training\n",
        "\n",
        "df = pd.read_csv(\"validated.tsv\", sep=\"\\t\")\n",
        "if df is not None:\n",
        "    # check if the 'clips' directory exists\n",
        "    clips_dir = \"clips\"\n",
        "    if not os.path.exists(clips_dir):\n",
        "        print(\"directory not found\")\n",
        "    else:\n",
        "        #output dir for wavs\n",
        "        os.makedirs(\"wav_clips\", exist_ok=True)\n",
        "        #convert mp3 to wav , create lists for dataset, this will hold full paths and transcripts for each tamil audio\n",
        "        audio_paths = []\n",
        "        transcripts = [] # tamil\n",
        "        engTransripts = [] # english\n",
        "        hindiTranscripts = [] # hindi\n",
        "\n",
        "        # loop will go through each entry in the tsv file and extract the path name\n",
        "        for index, row in df.iterrows():\n",
        "          mp3_file = os.path.join(clips_dir, row[\"path\"])\n",
        "          #construct input mp3 and output is wav\n",
        "          wav_file = os.path.join(\"wav_clips\", row[\"path\"].replace(\".mp3\", \".wav\"))\n",
        "\n",
        "          #convert only if wav does not already exist\n",
        "          if not os.path.exists(wav_file):\n",
        "            try:\n",
        "                #load the mp3 , set sample rate ( covert to mono) and export the file as wav\n",
        "                audio = AudioSegment.from_file(mp3_file)\n",
        "                audio = audio.set_frame_rate(16000).set_channels(1)  # 16kHz mono\n",
        "                audio.export(wav_file, format=\"wav\")\n",
        "            except FileNotFoundError:\n",
        "                 print(\"Mp3 file not found\")\n",
        "                 continue # skip this file and continue with the next\n",
        "          # i am populating the audio path and transcripts list\n",
        "          audio_paths.append(wav_file)\n",
        "          #row will be dict ( keys represent the column name)and sentence key holds the sentence value for that particular audio file\n",
        "          transcripts.append(row[\"sentence\"])\n",
        "          #translate to english\n",
        "          translator = Translator()\n",
        "          engTransripts.append(translator.translate(row[\"sentence\"], src='ta', dest='en').text)\n",
        "          translator = Translator()\n",
        "          hindiTranscripts.append(translator.translate(row[\"sentence\"], src='ta', dest='hi').text)\n",
        "\n",
        "        #now we can build dataframe and save as dataset with tamil audio and its tamil translation as specified in validation.tsv\n",
        "        dataset_df = pd.DataFrame({\n",
        "            \"audio\": audio_paths,\n",
        "            \"transcript_Tamil\": transcripts,\n",
        "            \"transcript_English\": engTransripts,\n",
        "            \"transcript_Hindi\": hindiTranscripts\n",
        "        })\n",
        "\n",
        "        dataset_df.to_csv(\"tamil_asr_dataset.csv\", index=False)\n",
        "        print(\"✅ Dataset saved as tamil_asr_dataset.csv\")\n",
        "        print(dataset_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'coroutine' object has no attribute 'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-1618975191.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0;31m#translate to english\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           \u001b[0mengTransripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m           \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mhindiTranscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'text'"
          ]
        }
      ]
    }
  ]
}
